---
title: "Characteristics of Recent Mexican Immigrants to the United States that Influence Household Income"
author: "Ben 10"
date: "12/05/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      message = FALSE, 
                      warning = FALSE)
```

```{r load-packages}
library(tidyverse)
library(broom)
library(leaps)
library(knitr)
library(dplyr)
library(rms)
```


```{r data}
data <- read_csv("/cloud/project/03-regression-analysis/data.csv")
```


Your project goes here! Before you submit, make sure your chunks are turned off with `echo = FALSE`. 

You can add sections as you see fit. At a minimum, you should have the following sections: 


## Section 1: Introduction (includes introduction and exploratory data analysis)
## 1. Introduction

### 1.1 Objective

We are aiming to build a model to determine which characteristics of Mexican immmigrants to the United States, specifically California, well-explain variation in household income. 

### 1.2 Description of Dataset

The dataset is from The Mexican Migration Project (MMP, *see References below for confidentiality terms). It was created in 1982 by an interdisciplinary team of researchers to further our understanding of the complex process of Mexican migration to the United States. The project is a binational research effort co-directed by Jorge Durand, professor of Social Anthropology at the University of Guadalajara (Mexico), and Douglas S. Massey, professor of Sociology and Public Affairs, with a joint appointment in the Woodrow Wilson School, at Princeton University (US).

Since its inception, the MMP's main focus has been to gather social as well as economic information on Mexican-US migration. The data collected has been compiled in a comprehensive database that is available to the public free of charge for research and educational purposes through its web-site. The MMP uses the ethnosurvey approach to gather data: in winter months, they randomly sample households in communities throughout Mexico, surveying household heads and members about their first and last trip to the US, as well as economic and demographic information. They then conduct the same survey in destination areas in the US, sampling migrants from the same communities they survey in Mexico but who have not returned to Mexico. Thus, the sample of migrants includes residents in both Mexico and the US.

The MMP170 Database contains an initial file with general demographic, economic, and migratory information for each member of a surveyed household (PERS). Pers170 has 132 variables and 176701 observations,and hence it is very large. Therefore, we selected 17 meaningful variables and filtered out rows that contain N/A's to create a new dataset labeled `data`.

### 1.3 Method

We will build a multiple linear regression model to predict household income considering the following variables: `sex`, `relhead`, `age`, `statebrn`, `marstat`, `edyrs`, `occtype`, `usdur1`, `usdurl`, `usdoc1`, `uscity`, `yrborn`. 

"X1": Number of observation

"sex": Sex
  
"relhead": Relationship to household head

"yrborn": Year of birth

"age": Age

"statebrn": State of birth

"marstat": Marital status

"edyrs": School years completed

"occ": Principal occupation

"hhincome" : Household income

"usstate1": First US migration: State of residence

"usstatel": Latest US migration: State of residence

"usplace1": First US migration: City of residence (in place codes)

"usplacel": Latest US migration: City of residence (in place codes)

"usdur1": First US migration: Duration (in months)

"usdurl": Latest US migration: Duration (in months)

"usdoc1": Type of documentation 

"occtype": Category of occupation

"uscity": City of residence during first US migration

Our response variable is household income: the total income for a single household, reported in $USD. We chose to use the multiple linear regression because our response variable is numeric, and there are multiple predictor variables.

## 2. Exploratory Data Analysis

### 2.1 Data Cleaning

Due to the complexity of our original data, we did not include data cleaning in the analysis. For more information, please see our proposal, where all the data cleaning happens.

However, we did make some adjustment according to the feedback that there is large imbalance of the amount of data between regions, and that the distribution of the response variable is not normal. Below is the update on our data cleaning:

### 2.2 Updated Data Exploration

#### 2.2.1 Filter Only Immigrants in California

Accoridng to our previous data exploration, we found that the overwhelming majority of immigrants settled in California, as shown in the graph below:

```{r}
ggplot(data = data, aes(x = uscity)) + 
  geom_bar() +
  coord_flip()+
  labs(title = "Distribution of City of Residence", x= "City of Residence", y = "Count")
```

Hence, we decided to concentrate on California alone. Since the originial dataset is large, we have enough data left in California alone to produce meaningful analysis.

```{r caldata}
caldata <- data %>%
  filter(usstatel == "California")

caldata <- caldata%>%
  na.omit()

```

#### 2.2.2 Cut Household Income Groups

Originally, the distribution of log(Household Income)- our response variable- was bimodal and had a median of 412,647 dollars. It almost looks like 3 separate distributions:

```{r nonfilt-income-plot}
ggplot(data = caldata, aes(x = log(hhincome))) +
  geom_histogram(binwidth = .7)+
  labs(title = "Distribution of Log(Household Income)", x= "log(Household Income)", y = "Count")
```

We determined that 412,647 dollars is an absurdly high median income for a survey of largely undocumented immigrants in the US and believe that a significant chunk of the high incomes were actually recorded in pesos. The documentation for the data from the Mexican Migration Project does not specify unit of hhincome; however, the project site details that researchers surveyed communities in Mexico, then traveled to the US to survey communities there. It seems likely that the communities surveyed in Mexico would report income in pesos and those surveyed in the US would report income in USD. However, the data was collected over a period of 10 years, during which the exchange rate between pesos and USD changed significantly. Hence, we cannot simply convert all the incomes that appear to be recorded in pesos into USD. 

Therefore, we decided to filter out the incomes above 60,000 to remove what appears to be a second distribution of incomes in pesos. We will also remove incomes of zero from our dataset, because it will interfere with our model accuracy. However, this compromises our model's predicative range: our model will only be able to predict the household income of those who already have jobs (income).

```{r filter-hhincome}
caldata <- caldata %>%
  mutate(hhincome = case_when(
    hhincome == .1 ~ 0,
    TRUE ~ hhincome
  )) #undoing mutation from proposal to return 0 income variables to 0
caldata <- caldata %>%
  filter(hhincome > 5, log(hhincome) < 8)
```


```{r hhincome-distribution}
ggplot(data = caldata, aes(x = hhincome)) +
  geom_histogram(binwidth = 100) +
  labs(title = "Distribution of Household Income", x= "Household Income", y = "Count")
```

Now the distribution of response variable (hhincome) looks like a right skewed normal distribution.

```{r cities, fig.align="center"}
caldata <- caldata %>%
  filter(uscity %in% c("Bakersfield, CA","Bakersfield, CA", "Fresno, CA", "Los Angeles-Long Beach, CA", "Merced, CA","Orange County, CA","Riverside-San Bernardino, CA", "Sacramento, CA", "San Diego, CA", "San Francisco, CA", "San Jose, CA","Santa Barbara-Santa Maria-Lompoc, CA","Santa Cruz-Watsonville, CA","Vallejo-Fairfield-Napa, CA","Ventura, CA"))
ggplot(data = caldata, aes(x = uscity))+ 
  geom_bar() +
  coord_flip()+
  labs(title = "Distribution of Cities in California", x = "City", y = "Count")
```

These immigrants to California arrived to the following cities: Los Angeles-Long Beach, San Francisco, San Diego, Santa Cruz-Watsonville, Bakersfield, Fresno, Merced, Orange County, Riverside-San Bernardino, Sacramento, San Jose, Santa Barbara-Santa Maria-Lompoc, Vallejo-Fairfield-Napa, and Ventura. Given the comparatively small number of cases in which no city was reported, we deleted these instances. This leaves 15 unique locations in California. The majority of immigrants went to LA-Long Beach area. 

#### 2.2.3 Remove Variable "relhead"

It turned out that all values from relhead (relationship to head of household) in our cleaned data were "1" or head. So we will remove this variable, as well as state variables since we are only using California data. We will also remove place data since we are using uscity, and occ since we are using occtype.

```{r remove-relhead}
caldata <- subset(caldata, select = -c(relhead, usstatel, usstate1,usplace1, usplacel,occ) )
```

#### 2.2.4 Mean-center "age" , "usdur1" and "usdurl"

We must center age and usdurl in order to have a useful model intercept interpretation.

```{r mean-age-usdurl}
mean(caldata$age) #mean age
mean(caldata$usdurl) #mean usdurl
mean(caldata$usdur1) #mean usdur1
mean(caldata$edyrs) #mean edyrs
```

The mean age in the dataset is 39.43 years ; the mean duration of last US migration is 60.27 months (about 5 years); and the mean duration of first US migration is 43.99 months (less than 3.5 years).

```{r age-usdurl-cent}
caldata <- caldata %>%
  mutate(age=age-mean(age))
caldata <- caldata %>%
  mutate(usdurl=usdurl-mean(usdurl))
caldata <- caldata %>%
  mutate(usdur1=usdur1-mean(usdur1))
```

## Section 2: Regression Analysis (includes the final model and discussion of assumptions)
## 2. Multiple Linear Regression Model

In an effort to explain which characteristics of migrants influence their household income, we will use a multiple linear regression model. Since our response variable is numerical with mulitple potential predictors, this is the best model at our disposal.

We will consider the potential interaction between principal occupation and number of years of school completed, since those are generally interconnected. We may also consider the interaction between documentation type and occupation type, although the effect may be insignificant. However, if the variables occtype, edyrs, or usdoc1 don't make it through the process of inital model selection, we will not include these interactions in the model as that would not be prudent.

We will select our model using AIC criteria, because since we're dealing with people, we want to build a model that accounts for volatile human nature and the ever-changing socioeconomic and political climate that could influence someone's household income. AIC is used when we would rather say a variable is a relevant predictor, when in reality it might not be and so in this case, we would rather err on the side of a false positive because we are dealing with a constantly fluctuating issue.

### 2.1 Full Model

#### 2.2.5 Remove Obvious Collinear Variable

```{r yrborn-age, fig.align = "center"}
ggplot(data = caldata, mapping = aes(x = yrborn, y = age)) +
  geom_point()+
  labs(title = "Correlation Between Age and Year Born", x = "Year Born", y = "Age")
```

`yrborn` and `age` provide the same information and are perfectly linear, therefore we decided to remove `yrborn` from consideration in the model.


## 3. Multiple Linear Regression Model

In an effort to explain which characteristics of candidates influence their household income, we will be using a multiple linear regression model. Since our response variable is numerical with mulitple potential predictors, this is the best model at our disposal for us to use.

We will consider the potential interaction between principal occupation and number of years of school completed, since those are generally interconnected. We may also consider the interaction between documentation type and occupation type, although the effect may be insignificant.

We will select our model using AIC criteria, because since we're dealing with people, we want to build a model that accounts for volatile human nature and the ever-changing socioeconomic and political climate that could influence someone's household income. AIC is used when we would rather say a variable is a relevant predictor, when in reality it might not be and so in this case, we would rather err on the side of a false positive because we are dealing with a constantly fluctuating issue.

### 3.1 Full Model

```{r model}
full <- lm(hhincome ~ sex + age + statebrn + marstat + edyrs + occtype + usdur1 + usdurl + usdoc1 + uscity, data=caldata)
kable(tidy(full),format="html" ,digits=3)
```

### 3.2 Backward selection

```{r backward-selection}
reduced <- step(full, direction = "backward")
sel_summary <- summary(reduced)
tidy(reduced, exponentiate = FALSE, conf.int = TRUE) %>%
kable(digits = 3, format = "markdown")
```

Using backward selection based on AIC, we narrowed down to 4 variables: sex, edyrs, usdurl and age.


### 3.3 Interactions

To find potential interactions between the 4 variables, we used nested-F test for each of the possible interactions:

After inital backwards selection, we will explore the possible interactions between remaining variables- edyrs, usdurl, sex, and age- to determine if any are significant.

```{r int-model}
int_model_1 <- lm(hhincome ~ sex + age + edyrs + usdurl + edyrs*usdurl, data = caldata)
anova(reduced, int_model_1, test = "Chisq") %>% 
  kable(format = "markdown", digits = 3)  # p = 0.653

int_model_2 <- lm(hhincome ~ sex + age + edyrs + usdurl + sex*usdurl, data = caldata)
anova(reduced, int_model_2, test = "Chisq") %>% 
  kable(format = "markdown", digits = 3) # p = 0.034

int_model_3 <- lm(hhincome ~ sex + age + edyrs + usdurl + age*usdurl, data = caldata)
anova(reduced, int_model_3, test = "Chisq") %>% 
  kable(format = "markdown", digits = 3)  # p = 0.201

int_model_4 <- lm(hhincome ~ sex + age + edyrs + usdurl + sex*edyrs, data = caldata)
anova(reduced, int_model_4, test = "Chisq") %>% 
  kable(format = "markdown", digits = 3)  # p = 0.326

int_model_5 <- lm(hhincome ~ sex + age + edyrs + usdurl + age*edyrs, data = caldata)
anova(reduced, int_model_5, test = "Chisq") %>% 
  kable(format = "markdown", digits = 3)  # p = 0

int_model_6 <- lm(hhincome ~ sex + age + edyrs + usdurl + age*sex, data = caldata)
anova(reduced, int_model_6, test = "Chisq") %>% 
  kable(format = "markdown", digits = 3)  # p = 0.376
```

Through nested F-test, we observed significant interactions between age & edyrs and between sex & usdurl, with respective p-values of 0 and 0.034.

### 3.4 Model with Interaction

```{r final-model}
reduced_int <- lm(hhincome ~ sex + age + edyrs + usdurl + age*edyrs + sex*usdurl, data=caldata)
kable(tidy(reduced_int),format="html" ,digits=3)
```

### 3.5 Backward Selection with Interaction

Since we observed 2 pairs of significant interactions, we will do the backward selection again with the new interaction terms.

```{r}
full_int <- lm(hhincome ~ sex + age + statebrn + marstat + edyrs + occtype + usdur1 + usdurl + usdoc1 + uscity + sex*usdurl + age*edyrs , data=caldata)
kable(tidy(full_int),format="html" ,digits=3)
```

```{r}
reduced_int_new <- step(full_int, direction = "backward")
sel_summary <- summary(reduced_int_new)
tidy(reduced_int_new, exponentiate = FALSE, conf.int = TRUE) %>%
kable(digits = 3, format = "markdown")
```

We observed that the variable selection changed. Specifically, usdur1 and usdoc1 are significant, besides the 4 variables and the 2 interaction terms identified previously.

### 3.6 High Leverage Points 

```{r augment}
mig_output <- augment(reduced_int_new) %>%
  mutate(obs_num = row_number())
```

.hat: leverage
.cooksd: Cook's distance
.std.resid: standardized residuals

```{r leverage}
leverage_threshold <- 2*(5+1)/nrow(caldata)

ggplot(data = mig_output, aes(x = obs_num,y = .hat)) + 
  geom_point(alpha = 0.7) + 
  geom_hline(yintercept = leverage_threshold,color = "red")
 ## labs(x = "Observation Number",y = "Leverage",title = "Leverage") +
 ## geom_text(aes(label=ifelse(.hat > leverage_threshold, as.character(obs_num), "")), nudge_x = 4)

mig_output %>% 
  filter(.hat > leverage_threshold)
145/nrow(caldata)
```

```{r standardized-residuals}
ggplot(data = mig_output, aes(x = .fitted,y = .std.resid)) +
  geom_point(alpha = 0.7) + 
  geom_hline(yintercept = 0,color = "red") +
  geom_hline(yintercept = -2,color = "red",linetype = "dotted") +
  geom_hline(yintercept = 2,color = "red",linetype = "dotted") +
  labs(x ="Predicted Value",y ="Standardized Residuals",title = "Standardized Residuals vs. Predicted") +
  geom_text(aes(label = ifelse(abs(.std.resid) >2,as.character(obs_num),"")), nudge_x = 0.3)

mig_output %>% 
  filter(abs(.std.resid) > 2)
```
  
Standardized residuals vs. predictors!!! make plots

  
Estimate of regression standard deviation, σ̂,  using all observations

```{r regression-standard-deviation}
glance(reduced_int_new)$sigma

mig_output %>%
  filter(abs(.std.resid) <= 2) %>%
  summarise(sigma_est = sqrt(sum(.resid^2)/(n() - 5 - 1)))
```

Estimate of σ̂ without points with large magnitude standardized residuals

tips_output %>%
  filter(abs(.std.resid) <= 2) %>%
  summarise(sigma_est = sqrt(sum(.resid^2)/(n() - 5 - 1)))
  
Recall that we use σ̂ to calculate the standard errors for all confidence intervals and p-values, so outliers can affect conclusions drawn from model

```{r cooks-distance}
ggplot(data = mig_output, aes(x = obs_num, y = .cooksd)) + 
  geom_point(alpha = 0.7) + 
  geom_hline(yintercept=1,color = "red")+
  labs(x= "Observation Number",y = "Cook's Distance",title = "Cook's Distance") +
  geom_text(aes(label = ifelse(.hat>1,as.character(obs_num),"")))
```

```{r vif}
tidy(vif(reduced_int_new))
```



## Section 3: Discussion and Limitations 

This section should include any relevant predictions and/or conclusions drawn from the model. Also critique your own methods and provide suggestions for improving your analysis. Issues pertaining to the reliability and validity of your data and appropriateness of the regression analysis should also be discussed here. A paragraph on what you would do differently if you were able to start over with the project or what you would do next if you were going to continue work on the project should also be included.


### 3.1 Prediction

### 3.1.1 Effect of "Gender" on Wage 

```{r}
x0 <- data.frame(sex = "M" , age = 0, edyrs = 6, usdur1 = 0, usdurl = 0, usdoc1 = "Undocumented")
predict.lm(reduced_int_new, x0, interval = "confidence")
```
For a male who is 39 years old (average age), has 6 years of education (average edyrs), first immigrated to the US for 5 years (average duration), and last immigrated to the US for 3 years and 7 months (average duration), and has a documentation type of "undocumented", his predicted wage is $850.11 We are 95% confident that the actual salary falls in the interval of [797.27, 902.95].

```{r}
x1 <- data.frame(sex = "F" , age = 0, edyrs = 6, usdur1 = 0, usdurl = 0, usdoc1 = "Undocumented")
predict.lm(reduced_int_new, x1, interval = "confidence")
```
For a female who is 39 years old (average age), has 6 years of education (average edyrs), first immigrated to the US for 5 years (average duration), and last immigrated to the US for 3 years and 7 months (average duration), and has a documentation type of "undocumented", his predicted salary is $620.83 We are 95% confident that the actual salary falls in the interval of [397.38, 844.29].

### 3.1.2 Effect of "Type of Documentation" on Wage

## Section 4: Conclusion
## Section 5: Additional Work



